{"cells":[{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Классификация дефектов кожи"},{"metadata":{},"cell_type":"markdown","source":"## Дипломная работа Жеманцевой Татьяны. Skillfactory."},{"metadata":{},"cell_type":"markdown","source":"## Задача: создание модели классификации, которая по представленной картинке кожного покрова сможет определить варианты с процентным \n## соотношением похожести данного дефекта на известное заболевание.\n \n## Данные взяты с kaggle - закрытое соревнование, данные для которого взяты и отсортированы в ISIC (https://www.isic-archive.com).\n## В данной работе выполнена классификация с использованием CNN и предобученой нейросети в Tensorflow - Xception."},{"metadata":{"trusted":true},"cell_type":"code","source":"#!pip install -q tensorflow==2.3\n#!pip install keras_efficientnets\n#!pip freeze > requirements.txt","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Импортируем библиотеки "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Импортируем библиотеки\n\nimport pandas as pd # импортируем необходимые библиотеки\nimport numpy as np\nimport re\nimport random\nimport os\nimport sys\nimport PIL\nimport cv2\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, ExtraTreesRegressor, BaggingRegressor\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.tree import ExtraTreeRegressor\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler \n\nfrom collections import Counter\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport itertools \n\n#from catboost import CatBoostRegressor\n\nfrom tqdm import tqdm\nfrom tqdm.notebook import tqdm\n\nfrom itertools import combinations\nfrom scipy.stats import ttest_ind\n\n\n# # keras\nimport tensorflow as tf\nfrom tensorflow import keras\n#import tensorflow.keras.layers as L\n#from tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n#import albumentations\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.applications.xception import Xception\n#from tensorflow.keras.layers import *\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Sequential\n#import tensorflow as tf\nimport tensorflow.keras \nimport tensorflow.keras.models #as M\n#import tensorflow.keras.layers as L\n#import tensorflow.keras.backend #as K\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay as ExpDecay\n\nfrom keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\nfrom glob import glob\nfrom glob import iglob\n#from keras.applications import InceptionV3\n#from keras.models import Sequential \nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense ,LeakyReLU,UpSampling2D, GlobalAveragePooling2D,Conv2DTranspose, BatchNormalization,GlobalMaxPool2D, Convolution2D\n\nimport nltk\nfrom nltk.corpus import stopwords\n\n# plt\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#увеличим дефолтный размер графиков\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10, 5\n#графики в svg выглядят более четкими\n%config InlineBackend.figure_format = 'svg' \n%matplotlib inline\n#DATA_PATH = \"/content/drive/My Drive/cars2/\"\n#PATH = \"/content/\"  # рабочая директория","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from tensorflow import keras\nfrom tensorflow.keras.applications.xception import Xception\nfrom tensorflow.keras.applications import EfficientNetB4\nprint(keras.__version__)\nprint(tf.__version__)","execution_count":29,"outputs":[{"output_type":"stream","text":"2.4.0\n2.4.0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tf.__version__)","execution_count":30,"outputs":[{"output_type":"stream","text":"2.4.0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#from tensorflow.keras.applications import InceptionV3#","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#keras.applications.EfficientNetB4()","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Python       :', sys.version.split('\\n')[0])\nprint('Numpmodel.summary()y        :', np.__version__)\nprint('Tensorflow   :', tf.__version__)","execution_count":33,"outputs":[{"output_type":"stream","text":"Python       : 3.7.9 | packaged by conda-forge | (default, Dec  9 2020, 21:08:20) \nNumpmodel.summary()y        : 1.19.5\nTensorflow   : 2.4.0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Задаем константы"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_path ='/kaggle/input/dermnet/train'\ntest_path = '/kaggle/input/dermnet/test'\n#train_path = \"/kaggle/input/skin-cancer9-classesisic/skin cancer isic the international skin imaging collaboration/Skin cancer ISIC The International Skin Imaging Collaboration/Train/\"\n#test_path = \"/kaggle/input/skin-cancer9-classesisic/skin cancer isic the international skin imaging collaboration/Skin cancer ISIC The International Skin Imaging Collaboration/Test/\"","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"RANDOM_SEED = 42\nIMAGE_SIZE         = (299, 299)\n#INPUT_SHAPE        = (224, 224, 3)\nIMG_SIZE           = 299 # размер входного изображения для Xception по-умолчанию\nIMG_CHANNELS       = 3   # у RGB 3 канала\ninput_shape        = (IMG_SIZE, IMG_SIZE, IMG_CHANNELS)\n# Keras settings\nBATCH_SIZE = 24\nVAL_BATCH_SIZE = 24\nVAL_SPLIT          = 0.1 # сколько данных выделяем на тест = 15%\nEPOCHS = 12\nLR = 0.00001","execution_count":35,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Предобработка изображений: просмотр, аугментация, создание обучающего и тестового наборов"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Посмотрим пример картинки\n\nimg = PIL.Image.open(train_path + \"nevus/ISIC_0000041.jpg\") # load_img(train_path + \"nevus/ISIC_0000041.jpg\")  \"ISIC_0000398.jpg\"\nplt.imshow(img)\nplt.axis(\"off\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Преобразуем картинку в массив\n\nx = img_to_array(img)\nprint(x.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# C помощью glob определяем сколько разных папок-классов с картинками имеется.\n\nclassName = glob(train_path + '/*' )\nnumberOfClass = len(className)\nprint(\"NumberOfClass: \",numberOfClass)","execution_count":36,"outputs":[{"output_type":"stream","text":"NumberOfClass:  23\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"className","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Аугментация с помощью ImageDataGenerator (выполняется либо этот раздел аугментации, либо следующий )"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Аугментация обучающего датасета\n# Официальная документация: https://keras.io/preprocessing/image/\n\ntrain_datagen = ImageDataGenerator(\n    rescale=1. / 255,\n    rotation_range = 60,\n    brightness_range = [0.5, 1.5],\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    validation_split=VAL_SPLIT, # set validation split\n    horizontal_flip=True,\n    shear_range=0.2,\n    zoom_range=[0.75,1.25],\n    fill_mode='reflect')  #  fill_mode='nearest'\n\n       \ntest_datagen = ImageDataGenerator(rescale=1. / 255)\nval_datagen = ImageDataGenerator(rescale=1. / 255, validation_split=VAL_SPLIT)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Завернем наши данные в генератор.\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_path,      # директория где расположены папки с картинками \n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=True, \n    seed=RANDOM_SEED,\n    subset='training') # set as training data\n\ntest_generator = val_datagen.flow_from_directory(\n    train_path,\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=True, \n    seed=RANDOM_SEED,\n    subset='validation') # set as validation data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_val_generator = test_datagen.flow_from_directory(\n    directory=test_path,\n    shuffle=False,\n    class_mode=None,\n    seed=RANDOM_SEED,\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_generator","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Аугментация с помощью albumentations (выполняется либо этот раздел аугментации, либо предыдущий)"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install albumentations","execution_count":12,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: albumentations in /opt/conda/lib/python3.7/site-packages (0.5.2)\nRequirement already satisfied: numpy>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from albumentations) (1.19.5)\nRequirement already satisfied: imgaug>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from albumentations) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from albumentations) (5.3.1)\nRequirement already satisfied: opencv-python-headless>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from albumentations) (4.5.1.48)\nRequirement already satisfied: scikit-image>=0.16.1 in /opt/conda/lib/python3.7/site-packages (from albumentations) (0.18.1)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from albumentations) (1.4.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (7.2.0)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (4.5.1.48)\nRequirement already satisfied: Shapely in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (1.7.1)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (3.3.3)\nRequirement already satisfied: imageio in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (2.9.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations) (1.15.0)\nRequirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2.5)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\nRequirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2021.1.14)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.4.7)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (0.10.0)\nRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (2.8.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations) (1.3.1)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations) (4.4.2)\n\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install git+https://github.com/mjkvaak/ImageDataAugmentor","execution_count":13,"outputs":[{"output_type":"stream","text":"Collecting git+https://github.com/mjkvaak/ImageDataAugmentor\n  Cloning https://github.com/mjkvaak/ImageDataAugmentor to /tmp/pip-req-build-1vuwvk5v\n  Running command git clone -q https://github.com/mjkvaak/ImageDataAugmentor /tmp/pip-req-build-1vuwvk5v\nRequirement already satisfied: opencv-python>=4.2 in /opt/conda/lib/python3.7/site-packages (from ImageDataAugmentor==0.0.0) (4.5.1.48)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from ImageDataAugmentor==0.0.0) (3.3.3)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from ImageDataAugmentor==0.0.0) (7.2.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from ImageDataAugmentor==0.0.0) (1.4.1)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from ImageDataAugmentor==0.0.0) (1.1.5)\nRequirement already satisfied: numpy>=1.14.5 in /opt/conda/lib/python3.7/site-packages (from opencv-python>=4.2->ImageDataAugmentor==0.0.0) (1.19.5)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.7/site-packages (from matplotlib->ImageDataAugmentor==0.0.0) (2.4.7)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->ImageDataAugmentor==0.0.0) (0.10.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->ImageDataAugmentor==0.0.0) (1.3.1)\nRequirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->ImageDataAugmentor==0.0.0) (2.8.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib->ImageDataAugmentor==0.0.0) (1.15.0)\nRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->ImageDataAugmentor==0.0.0) (2019.3)\nBuilding wheels for collected packages: ImageDataAugmentor\n  Building wheel for ImageDataAugmentor (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ImageDataAugmentor: filename=ImageDataAugmentor-0.0.0-py3-none-any.whl size=29531 sha256=2c6a20155994ad4e0b2d52e6ddd302a025fcd8017be31b6f73cf5c9d52da048f\n  Stored in directory: /tmp/pip-ephem-wheel-cache-m_oas689/wheels/c9/bd/73/9cfa59d2393dae55bbcc30f5aa901f55fe531c66efebbc8fc3\nSuccessfully built ImageDataAugmentor\nInstalling collected packages: ImageDataAugmentor\nSuccessfully installed ImageDataAugmentor-0.0.0\n\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Аугментация данных важна, когда мы работаем с небольшим датасетом. Это как раз наш случай.\n\nimport albumentations\nimport cv2\nfrom ImageDataAugmentor.image_data_augmentor import *\nfrom albumentations import (\n    HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine,\n    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose\n)","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# попробовать этот вариант\n\nAUGMENTATIONS = albumentations.Compose([\n    albumentations.HorizontalFlip(p=0.5), #0.5\n    albumentations.Rotate(limit=90),\n    albumentations.JpegCompression(quality_lower=85, quality_upper=100, p=0.5),\n    albumentations.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n   # albumentations.HueSaturationValue(p=0.5),\n    albumentations.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.2, hue=0.00, always_apply=False, p=0.5),\n    albumentations.RandomContrast(limit=0.1, p=0.5),\n    albumentations.Resize(IMG_SIZE, IMG_SIZE)\n])\n\ntrain_datagen = ImageDataAugmentor(\n        rescale=1./255,\n        augment = AUGMENTATIONS,\n        validation_split=VAL_SPLIT,\n        )\n\n      \ntest_datagen = ImageDataAugmentor(rescale=1./255)\n\nval_datagen = ImageDataAugmentor(rescale=1. / 255, validation_split=VAL_SPLIT)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Либо этот вариант. \n\n\nAUGMENTATIONS = albumentations.Compose([\n    albumentations.HorizontalFlip(p=0.5), #0.5\n    albumentations.Rotate(limit=20, interpolation=1, border_mode=4, value=None, mask_value=None, always_apply=False, p=0.5),\n    albumentations.OneOf([\n        albumentations.CenterCrop(height=IMG_SIZE, width=IMG_SIZE),\n        albumentations.CenterCrop(height=IMG_SIZE, width=IMG_SIZE),\n    ],p=0.5),\n    albumentations.OneOf([\n        albumentations.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3),\n        albumentations.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1)\n    ],p=0.5),\n    albumentations.GaussianBlur(p=0.05),\n    albumentations.HueSaturationValue(p=0.5),\n    albumentations.RGBShift(p=0.5),\n   # albumentations.FancyPCA(alpha=0.1, always_apply=False, p=0.5),\n    albumentations.Resize(IMG_SIZE, IMG_SIZE)\n])\n\ntrain_datagen = ImageDataAugmentor(\n        rescale=1./255,\n        augment = AUGMENTATIONS,\n        validation_split=VAL_SPLIT,\n        )\n\n      \ntest_datagen = ImageDataAugmentor(rescale=1./255)\n\nval_datagen = ImageDataAugmentor(rescale=1. / 255, validation_split=VAL_SPLIT)","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Завернем наши данные в генератор:\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_path,      # директория где расположены папки с картинками \n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=True, seed=RANDOM_SEED,\n    subset='training') # set as training data\n\ntest_generator = val_datagen.flow_from_directory(\n    train_path,\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    class_mode='categorical',\n    shuffle=True, seed=RANDOM_SEED,\n    subset='validation') # set as validation data\n\n","execution_count":39,"outputs":[{"output_type":"stream","text":"Found 14011 images belonging to 23 classes.\nFound 1546 images belonging to 23 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_val_generator = test_datagen.flow_from_directory(\n    directory=test_path,\n    shuffle=False,\n    class_mode=None,\n    seed=RANDOM_SEED,\n    target_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE)","execution_count":40,"outputs":[{"output_type":"stream","text":"Found 4002 images belonging to 23 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Переведем картинки в массивы\n\ntrain_pairs = [next(train_generator) for i in range(len(train_generator))]\nval_pairs = [next(test_generator) for i in range(len(test_generator))]\ntrain_images = np.concatenate([x[0] for x in train_pairs], axis = 0)\ntrain_labels = np.concatenate([x[1] for x in train_pairs], axis = 0)\nval_images = np.concatenate([x[0] for x in val_pairs], axis = 0)\nval_labels = np.concatenate([x[1] for x in val_pairs], axis = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#val_df=val_labels.flatten() \ntr_df = train_labels.flatten() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Прсмотрим несколько картинок после применения аугментации.\n\nfrom skimage import io\n\ndef imshow(image_RGB):\n    io.imshow(image_RGB)\n    io.show()\n\nx,y = train_generator.next()\nprint('Пример картинок из train_generator')\nplt.figure(figsize=(12,8))\n\nfor i in range(0,6):\n    image = x[i]\n    plt.subplot(3,3, i+1)\n    plt.imshow(image)\n   # plt.title('Class: '+str(y[i]))\n    #plt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imgs, labels = next(train_generator)\nprint(imgs.shape, labels.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imgs, labels = next(test_generator)\nprint(imgs.shape, labels.shape)\nprint(len(test_generator))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Посмотрим распределение данных по классам."},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef classes_count (path):\n    # определяет содержимое папок классов\n    \n    classes = []\n    for filename in iglob(os.path.join(path, \"**\",\"*.jpg\")):\n        classes.append(os.path.split(os.path.split(filename)[0])[-1])\n    cnt = Counter(classes)\n    return cnt\n\ndef percentage_value(pct, allvals):\n    # вычисляет процентное значение размера класса\n    \n    absolute = int(pct/100.*np.sum(allvals))\n    return \"{:.1f}%\\n({:d})\".format(pct, absolute)\n\ndef plot_dataset_description(path, title):\n    # строит круговую диаграмму\n    \n    classes_cnt = classes_count(path)\n    values = list(classes_cnt.values())\n    labels = list(classes_cnt.keys())\n\n    plt.figure(figsize=(8,8))\n    plt.pie(values, labels=labels, autopct=lambda pct: percentage_value(pct, values), \n            shadow=True, startangle=140)\n\n    plt.title(title + ' размера ' + str(sum(classes_cnt.values())))    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dataset_description(os.path.join(train_path), \"Содержание обучающей выборки\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dataset_description(os.path.join(test_path), \"Содержание тестовой выборки\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_dataset_description_bar(path, title):\n    # строит диаграмму столбиками\n \n    classes_val = classes_count(path)\n    \n\n    values = list(classes_val.values())\n    labels = list(classes_val.keys())\n    #print (labels)\n    #index = [n for n in range(numberOfClass)]\n    plt.figure(figsize=(10, 5))\n    #hist, bins = np.histogram(values, bins=9)\n    plt.bar(values, (values), width=10)\n #   plt.bar(range(len(values)),[values[k] for k in values], width=20)\n    \n    #plt.xlabel('Градация болезней', fontsize=10)\n    plt.ylabel('Количество картинок', fontsize=10)\n    plt.xticks(values, labels, fontsize=8, rotation=90)\n    plt.title(title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dataset_description_bar(os.path.join(train_path), 'Содержание примеров в обучающей выборке')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dataset_description_bar(os.path.join(test_path), 'Содержание примеров в тестовой выборке')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Анализ данных показывает, что наша выборка несбалансирована, поскольку самый многочисленный класс в 5 раз превышает самый маленький.\n# В этом случае нам подошли бы метрики precision, recall, F1-score, учитывающие веса классов, но они используются только в бинарной классификации. А проблема accuracy в том, что она не учитывает веса классов.\n# Поэтому для каждого класса добавлены веса классов в параметр class_weight при обучении,что позволило повысить точность модели.\n# В данной задаче, как и большинстве задач, связанных с медициной наиболее критичной является ошибка первого рода.\n\n \n# В задаче для оценки эффективности  модели в процессе обучения я использовала\n# 1. accurancy - без использования class_weight максимальный результат достигал порядка 56%\n# 2. Для многоклассовой выборки с несбалансированными классами на просторах интернета рекомендуют использовать метрику macro F1. Создала свои функции для подсчета loss и macro F1 - решение давало достаточно низкую степень обучаемости, от идеи отказалась. на 45 эпохе результаты: loss: 1.0659 - f1: 0.2935 - val_loss: 1.8877 - val_f1: 0.1625\n# Также попробовала оптимизацию по созданной функции потерь F1_loss - на 40й эпохе результат F1 = 0.23.\n\n# 3. Оптимизировала модель по loss - категориальная кросс энтропия. \n\n \n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Построение и обучение модели."},{"metadata":{"trusted":true},"cell_type":"code","source":"#model = keras.models.load_model('/kaggle/input/inputmod/best_model_step1.hdf5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Вынесем функции для раздела построения и обучения модели."},{"metadata":{"trusted":true},"cell_type":"code","source":"def info_layers():\n    # вывод информации по слоям модели в процессе тюнинга\n    \n    print(\"Number of layers in the model:\", len(model.layers))\n    print(\"Number of trainable_variables layers in the model:\", len(model.trainable_variables))\n    print(\"Number of layers in the base model: \", len(base_model.layers))\n    print(\"Number of variables layers in the base model: \", len(base_model.trainable_variables))\n    print(\"Layers in the model:\")\n    for layer in model.layers:\n        print(layer, layer.trainable)\n\ndef graph(history):\n    # построние графиков изменения метрики и функции потерь в процессе обучения по эпохам\n    \n    plt.figure(figsize=(8, 4))\n    plt.plot(history.history['accuracy'], 'b*-', label=\"training acc\")\n    plt.plot(history.history['val_accuracy'], 'r*-', label=\"validation acc\")\n    plt.grid()\n    plt.title(\"train_acc vs val_acc\")\n    plt.ylabel(\"Accuracy\")\n    plt.xlabel(\"Epochs\")\n    plt.legend()\n\n    plt.figure(figsize=(8, 4))\n    plt.plot(history.history['loss'], 'b*-', label=\"training loss\")\n    plt.plot(history.history['val_loss'], 'r*-', label=\"validation loss\")\n    plt.grid()\n    plt.title(\"Training and validation loss\")\n    plt.ylabel(\"Loss\")\n    plt.xlabel(\"Epochs\")\n    plt.legend()\n    plt.show()\n    \n    \n","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Создадим собственную метрику f1 и функцию потерь - f1_loss\n\nfrom keras import backend as K\n\ndef f1(y_true, y_pred):\n    # метрика macro F1\n    \n    y_pred = K.round(y_pred)\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp / (tp + fp + K.epsilon())\n    r = tp / (tp + fn + K.epsilon())\n\n    f1 = 2*p*r / (p+r+K.epsilon())\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)\n\ndef f1_loss(y_true, y_pred):\n    # функция потерь\n    \n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp / (tp + fp + K.epsilon())\n    r = tp / (tp + fn + K.epsilon())\n\n    f1 = 2*p*r / (p+r+K.epsilon())\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return 1 - K.mean(f1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CallBack для сохранения модели, ее весов и лучшей модели, изменения LR в случае отсутствия прогресса обучения, и остановка обучения\n\nweightpath = \"best_model.hdf5\"\n#checkpoint = ModelCheckpoint(weightpath , monitor = 'val_f1' , mode = 'max', verbose = 1,save_best_only=True)\n#checkpoint = ModelCheckpoint(weightpath , monitor = 'val_precision' , mode = 'max', verbose = 1,save_best_only=True) #save_weights_only=True\ncheckpoint = ModelCheckpoint(weightpath , monitor = 'val_accuracy' , mode = 'max', verbose = 1,save_best_only=True) #save_weights_only=True\nreducer = ReduceLROnPlateau(monitor='loss', patience= 3,factor=0.2, min_lr=0.00000001,  verbose = 1, cooldown=2, mode='min',min_delta=0.0000001) #min_delta=0.000000001\nearly_stop= EarlyStopping(monitor='loss',patience= 10, mode='min')  #min_delta=0.000000001\n","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"callbacks_list = [checkpoint, reducer, early_stop] ","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Выведем классы для удобства проставления весов\n\nclasses = {v: k for k, v in train_generator.class_indices.items()}\nprint(classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Установим вес \"2\"и \"3\" для классов, размер которых в 2-3 раза меньше самых больших классов, сделав эти классы в 2-3 раза важнее\n\nclass_weight = {0: 1., 1: 0.6, 2: 1., 3: 1., 4: 1., 5: 0.6, 6: 1., 7: 1., 8: 1.,9: 1., 10: 1., 11: 1., 12: 0.6, 13: 1., 14: 0.6, 15: 1., 16: 0.6, 17: 1.,18: 0.6, 19: 1., 20: 1., 21: 1., 22: 1.}","execution_count":44,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Шаг1  Сформируем модель"},{"metadata":{"trusted":true},"cell_type":"code","source":"# В процессе обучения проводились опыты с разными моделями, в качестве base_model необходимо выбрать  Xception, \n# как показавшую лучшие результаты\n\n#base_model2 = EfficientNetB4(weights='imagenet', include_top=False, input_shape=input_shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model = Xception(weights='imagenet', include_top=False, input_shape=input_shape)","execution_count":45,"outputs":[{"output_type":"stream","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n83689472/83683744 [==============================] - 1s 0us/step\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Установим все слои base_model в неактивное состояние\n\nbase_model.trainable = False","execution_count":46,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Sequential()\nmodel.add(base_model)\n\n#model.add(GlobalAveragePooling2D(),)\n#model.add(BatchNormalization())\nmodel.add(Conv2D(32, (3, 3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D())\nmodel.add(Dropout(0.15))\nmodel.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n#model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D())\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(128, (3, 3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D())\nmodel.add(Dropout(0.2))\n#model.add(BatchNormalization())\nprint('ок')\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\n#model.add(BatchNormalization())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.2))\n#model.add(L.Dense(512,activation='relu'))\n#model.add(L.BatchNormalization())\nmodel.add(Dense(numberOfClass, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# лучшая модель \n\nmodel=Sequential()\n##model.add(base_model)\n\n##model.add(BatchNormalization())\nmodel.add(Conv2D(64, (3, 3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D())\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(256, (3, 3), padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D())\nprint('ок')\nmodel.add(Flatten())\n#model.add(Dense(1024, activation='relu'))\n#model.add(Dropout(0.2))\n#model.add(BatchNormalization())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.2))\n#model.add(L.Dense(512,activation='relu'))\n#model.add(L.BatchNormalization())\nmodel.add(Dense(numberOfClass, activation='softmax'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_generator.samples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_generator.samples","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Xception(include_top = False, weights = \"imagenet\",\n                        input_shape=(IMG_SIZE,IMG_SIZE, 3)))\nmodel.add(GlobalAveragePooling2D())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(numberOfClass, activation = \"softmax\"))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CNN модель с добавлением Xception, которая показала наилучший результат \n\nmodel = Sequential()\n \nmodel.add(base_model)\nmodel.add(GlobalAveragePooling2D())\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(numberOfClass, activation='softmax'))\n\n","execution_count":47,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Посмотрим информацию о количестве слоев модели и модели Imagenet\n\ninfo_layers()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model.trainable = True\n\n# Fine-tune from this layer onwards\nfine_tune_at = 102\n\n# Freeze all the layers before the `fine_tune_at` layer\nfor layer in base_model.layers[:fine_tune_at]:\n  layer.trainable =  False","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"info_layers()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Посмотрим структуру модели\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install --upgrade pip","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pydot","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Посмотрим на схему модели.\n\nfrom keras.utils import plot_model\nplot_model(model,show_shapes=True,show_layer_names=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Компилируем модель\n\n#f1_m = 2*((tf.keras.metrics.Precision()*tf.keras.metrics.Recall())/(tf.keras.metrics.Precision()+tf.keras.metrics.Recall()+tf.epsilon()))\nLR=0.0008\nmodel.compile(optimizer=optimizers.Adam(LR),loss='categorical_crossentropy', metrics=['accuracy'])\n#model.compile(optimizer=optimizers.Adam(LR),loss='categorical_crossentropy', metrics=[tf.keras.metrics.Precision()])\n#model.compile(optimizer=optimizers.Adam(LR),loss=f1_loss, metrics=f1)\n#model.compile(optimizer=optimizers.Adam(LR),loss='categorical_crossentropy', metrics=f1)","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nhistory = model.fit(\n        train_generator,\n        #train_images, train_labels,\n        steps_per_epoch=len(train_generator), #train_generator.samples, # train_generator.samples # len(train_generator)\n        epochs= 56, #EPOCHS,\n        class_weight=class_weight,\n        validation_data=test_generator,\n        #validation_data= val_images,\n        #validation_steps=len(test_generator), #test_generator.samples,  # test_generator.samples\n        callbacks = callbacks_list)","execution_count":null,"outputs":[{"output_type":"stream","text":"Epoch 1/56\n  6/584 [..............................] - ETA: 3:54 - loss: 2.5882 - accuracy: 0.0654","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"history.history\n#loss: 0.1100 - precision: 0.9824 - val_loss: 4.3982 - val_precision: 0.4840   -- 0.00008\n#loss: 1.6093 - precision_1: 0.7621 - val_loss: 1.9980 - val_precision_1: 0.6232  -- LR=0.0007\n#loss: 1.8622 - precision_2: 0.6718 - val_loss: 1.7129 - val_precision_2: 0.6943  --0.001\n#loss: 1.5776 - precision_2 0.8497 - val_loss: 2.0402 - val_precision_3: 0.5302  --0.00017","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# сохраним итоговую сеть и подгрузим лучшую итерацию в обучении (best_model)\n\nmodel.save('model_last_9v1_23.hdf5')\nmodel.load_weights('best_model.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model = keras.models.load_model('model_last_111.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"graph(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Шаг2 Изменим количество неактивных слоев в сети imagenet и дообучим модель"},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model.trainable = True\n\n# Fine-tune from this layer onwards\nfine_tune_at = 80\n\n# Freeze all the layers before the `fine_tune_at` layer\nfor layer in base_model.layers[:fine_tune_at]:\n  layer.trainable =  False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Посмотрим информацию о количестве слоев модели \n\ninfo_layers()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Посмотрим структуру модели\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Компилируем модель\n\nLR=0.00075\nmodel.compile(optimizer=optimizers.Adam(LR),loss='categorical_crossentropy', metrics=['accuracy'])\n#model.compile(optimizer=optimizers.Adam(LR),loss=f1_loss, metrics=f1)\n#model.compile(optimizer=optimizers.Adam(LR),loss='categorical_crossentropy', metrics=[tf.keras.metrics.Precision()])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scores = model.evaluate_generator(test_generator, steps=len(test_generator), verbose=1)\n#print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nhistory = model.fit(\n        train_images, train_labels,\n        steps_per_epoch=len(train_generator), #train_generator.samples, # train_generator.samples # len(train_generator)\n        epochs= 50, #EPOCHS,\n        class_weight=class_weight,\n        validation_data=test_generator,\n        validation_steps=len(test_generator), #test_generator.samples,  # test_generator.samples\n        callbacks = callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# сохраним итоговую сеть и подгрузим лучшую итерацию в обучении (best_model)\n\nmodel.save('model_last_v2.hdf5')\nmodel.load_weights('best_model.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"graph(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Шаг3\n#model = keras.models.load_model('/kaggle/input/inputqw/model_last_017_83.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Шаг3 Снова изменим количество неактивных слоев в сети imagenet и дообучим модель","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model.trainable = True\n\n# Fine-tune from this layer onwards\nfine_tune_at = 31\n\n# Freeze all the layers before the `fine_tune_at` layer\nfor layer in base_model.layers[:fine_tune_at]:\n  layer.trainable =  False\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Посмотрим информацию о количестве слоев модели \n\ninfo_layers()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Посмотрим структуру модели\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Компилируем модель\n\nLR=0.00095\nmodel.compile(optimizer=optimizers.Adam(LR),loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#scores = model.evaluate_generator(test_generator, steps=len(test_generator), verbose=1)\n#print(\"Accuracy: %.2f%%\" % (scores[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nhistory = model.fit(\n        train_images, train_labels,\n        steps_per_epoch=len(train_generator), #train_generator.samples, # train_generator.samples # len(train_generator)\n        epochs= 48, #EPOCHS,\n        class_weight=class_weight,\n        validation_data=test_generator,\n        validation_steps=len(test_generator), #test_generator.samples,  # test_generator.samples\n        callbacks = callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# сохраним итоговую сеть и подгрузим лучшую итерацию в обучении (best_model)\n\nmodel.save('model_last_v3.hdf5')\nmodel.load_weights('best_model.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"graph(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Шаг4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model.trainable = True\n\n# Fine-tune from this layer onwards\nfine_tune_at = 11\n\n# Freeze all the layers before the `fine_tune_at` layer\nfor layer in base_model.layers[:fine_tune_at]:\n  layer.trainable =  False\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Посмотрим информацию о количестве слоев модели \n\ninfo_layers()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Посмотрим структуру модели\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Компилируем модель\n\nLR=0.00005\nmodel.compile(optimizer=optimizers.Adam(LR),loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nhistory = model.fit(\n        train_images, train_labels,\n        steps_per_epoch=train_generator.samples, #train_generator.samples, # train_generator.samples # len(train_generator)\n        epochs= 45, #EPOCHS,\n        class_weight=class_weight,\n        validation_data=test_generator,\n        validation_steps=(test_generator.samples), #test_generator.samples,  # test_generator.samples\n        callbacks = callbacks_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# сохраним итоговую сеть и подгрузим лучшую итерацию в обучении (best_model)\n\nmodel.save('model_last_19.hdf5')\nmodel.load_weights('best_model.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"graph2(history)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Результаты"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Считаем финальную точность модели по Accuracy и Loss\n\nscore_loss, score_acc = model.evaluate_generator(test_generator, steps=len(test_generator), verbose=1)\nprint(\"F1: %.2f%%\" % (score_acc*100))\nprint(\"Loss: %.2f%%\" % (score_loss))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Confugn matrix по тестовым данным"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \n    # функция рисования confusion_matrix\n\n    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure(figsize=(12,12))\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=18)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45, fontsize=8)\n    plt.yticks(tick_marks, classes, fontsize=12)\n\n    fmt = '.2f'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label', fontsize=16)\n    plt.xlabel('Predicted label', fontsize=16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nY_pred = model.predict(test_generator)  # test_val_generator\nY_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred2 = np.argmax(Y_pred, axis=1)\ny_pred2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnf_matrix = confusion_matrix(test_generator.classes, y_pred2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = {v: k for k, v in train_generator.class_indices.items()}\nprint(classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(classes.values())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(cnf_matrix, list(classes.values()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# классификация ответов\n\nprint(classification_report(test_generator.classes, y_pred2, target_names=list(classes.values())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Confugn matrix по валидационным данным","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nY_pred1 = model.predict(test_val_generator,verbose=1)  # test_val_generator\ny_pred_val = np.argmax(Y_pred1, axis=1)\ny_pred_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cnf_matrix_val = confusion_matrix(test_val_generator.classes, y_pred_val)\nclasses = {v: k for k, v in test_val_generator.class_indices.items()}\nprint(classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(cnf_matrix_val, list(classes.values()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# классификация ответов\n\nprint(classification_report(test_val_generator.classes, y_pred_val, target_names=list(classes.values())))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Tестовая демонстрация."},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_image(filename):\n    # загружает картинку\n    \n    img = cv2.imread(os.path.join(train_path, filename))\n    img = cv2.resize(img, (IMAGE_SIZE[0], IMAGE_SIZE[1]) )\n    img = img /255\n    return img\n\ndef predict(image):\n    # возвращает первые три максимальных значений из предсказанных для каждого класса\n    tta_step = 3\n    predictions = []\n    for i in range(tta_step):\n        preds = model.predict(np.asarray([img]))[0]\n        predictions.append(preds)\n    pred = np.mean(predictions, axis=0)\n    probabilities = pred\n#    probabilities = model.predict(np.asarray([img]))[0]\n    class_idx = (-probabilities).argsort()[:3]  # np.argmax(probabilities)\n    return class_idx, probabilities[class_idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = {v: k for k, v in train_generator.class_indices.items()}\nprint(classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx, filename in enumerate(random.sample(test_generator.filenames, 3)):    # test_val_generator\n    print(\"Источник: class: %s, file: %s\" % (os.path.split(filename)[0], filename))\n    \n    img = load_image(filename)\n    prediction = predict(img)\n    print(\"Предсказано: класс: %s, с вероятностью: %f\" % (classes[prediction[0][0]], prediction[1][0]))\n    print(\"Предсказано: класс: %s, с вероятностью: %f\" % (classes[prediction[0][1]], prediction[1][1]))\n    print(\"Предсказано: класс: %s, с вероятностью: %f\" % (classes[prediction[0][2]], prediction[1][2]))\n    plt.imshow(img)\n    plt.figure(idx)    \n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Выводы:\n#     На несбалансированной выборке с небольшим набором данных удалось добиться точности за счет следующих шагов:\n#         1. выбор подходящей к задаче архитектуры сети c Fine-tuning\n#         2. задание весов для классов\n#         3. уменьшение batch_size\n#         4. подбор LR\n#         5. использование аугментации ablumentation\n        "}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}